{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mxyar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mxyar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mxyar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "from tqdm import tqdm\n",
    "from langdetect import detect\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.pipeline import FeatureUnion,Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сlass, that converts the original tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FreeTextConverter:\n",
    "    def __init__(self, regex_ignore=[], emoticons_ignore=False):\n",
    "        self.regex_dict = {\n",
    "            'URL': r\"\"\"(?xi)\\b(?:(?:https?|ftp|file):\\/\\/|www\\.|ftp\\.|pic\\.|twitter\\.|facebook\\.)(?:\\([-A-Z0-9+&@#\\/%=~_|$?!:;,.]*\\)|[-A-Z0-9+&@#\\/%=~_|$?!:;,.])*(?:\\([-A-Z0-9+&@#\\/%=~_|$?!:,.]*\\)|[A-Z0-9+&@#\\/%=~_|$])\"\"\",\n",
    "            'EMOJI': u'([\\U0001F1E0-\\U0001F1FF])|([\\U0001F300-\\U0001F5FF])|([\\U0001F600-\\U0001F64F])|([\\U0001F680-\\U0001F6FF])|([\\U0001F700-\\U0001F77F])|([\\U0001F800-\\U0001F8FF])|([\\U0001F900-\\U0001F9FF])|([\\U0001FA00-\\U0001FA6F])|([\\U0001FA70-\\U0001FAFF])|([\\U00002702-\\U000027B0])|([\\U00002600-\\U000027BF])|([\\U0001f300-\\U0001f64F])|([\\U0001f680-\\U0001f6FF])',\n",
    "            'HASHTAG': r\"\\#\\b[\\w\\-\\_]+\\b\",\n",
    "            'EMAIL': r\"(?:^|(?<=[^\\w@.)]))(?:[\\w+-](?:\\.(?!\\.))?)*?[\\w+-]@(?:\\w-?)*?\\w+(?:\\.(?:[a-z]{2,})){1,3}(?:$|(?=\\b))\",\n",
    "            'MENTION': r\"@[A-Za-z0-9]+\",\n",
    "            'CASHTAG': r\"(?:[$\\u20ac\\u00a3\\u00a2]\\d+(?:[\\\\.,']\\d+)?(?:[MmKkBb](?:n|(?:il(?:lion)?))?)?)|(?:\\d+(?:[\\\\.,']\\\\d+)?[$\\u20ac\\u00a3\\u00a2])\",\n",
    "            'DATE': r\"(?:(?:(?:(?:(?<!:)\\b\\'?\\d{1,4},? ?)?\\b(?:[Jj]an(?:uary)?|[Ff]eb(?:ruary)?|[Mm]ar(?:ch)?|[Aa]pr(?:il)?|May|[Jj]un(?:e)?|[Jj]ul(?:y)?|[Aa]ug(?:ust)?|[Ss]ept?(?:ember)?|[Oo]ct(?:ober)?|[Nn]ov(?:ember)?|[Dd]ec(?:ember)?)\\b(?:(?:,? ?\\'?)?\\d{1,4}(?:st|nd|rd|n?th)?\\b(?:[,\\\\/]? ?\\'?\\d{2,4}[a-zA-Z]*)?(?: ?- ?\\d{2,4}[a-zA-Z]*)?(?!:\\d{1,4})\\b))|(?:(?:(?<!:)\\b\\\\'?\\d{1,4},? ?)\\b(?:[Jj]an(?:uary)?|[Ff]eb(?:ruary)?|[Mm]ar(?:ch)?|[Aa]pr(?:il)?|May|[Jj]un(?:e)?|[Jj]ul(?:y)?|[Aa]ug(?:ust)?|[Ss]ept?(?:ember)?|[Oo]ct(?:ober)?|[Nn]ov(?:ember)?|[Dd]ec(?:ember)?)\\b(?:(?:,? ?\\'?)?\\d{1,4}(?:st|nd|rd|n?th)?\\b(?:[,\\\\/]? ?\\'?\\d{2,4}[a-zA-Z]*)?(?: ?- ?\\d{2,4}[a-zA-Z]*)?(?!:\\d{1,4})\\b)?))|(?:\\b(?<!\\d\\\\.)(?:(?:(?:[0123]?[0-9][\\\\.\\\\-\\\\/])?[0123]?[0-9][\\\\.\\\\-\\\\/][12][0-9]{3})|(?:[0123]?[0-9][\\\\.\\\\-\\\\/][0123]?[0-9][\\\\.\\\\-\\\\/][12]?[0-9]{2,3}))(?!\\.\\d)\\b))\",\n",
    "            'TIME': r'(?:(?:\\d+)?\\\\.?\\d+(?:AM|PM|am|pm|a\\\\.m\\\\.|p\\\\.m\\\\.))|(?:(?:[0-2]?[0-9]|[2][0-3]):(?:[0-5][0-9])(?::(?:[0-5][0-9]))?(?: ?(?:AM|PM|am|pm|a\\\\.m\\\\.|p\\\\.m\\\\.))?)',\n",
    "            'EMPHASIS': r\"(?:\\*\\b\\w+\\b\\*)\",\n",
    "            'ELONG': r\"\\b[A-Za-z]*([a-zA-Z])\\1\\1[A-Za-z]*\\b\"}\n",
    "        \n",
    "        for key in regex_ignore:\n",
    "            if key in self.regex_dict:\n",
    "                del self.regex_dict[key]\n",
    "\n",
    "        self.contraction_mapping = {\"’\": \"'\", \"RT \": \" \", \"ain't\": \"is not\", \"aren't\": \"are not\", \"can't\": \"can not\",\n",
    "                                    \"'cause\": \"because\", \"could've\": \"could have\",\n",
    "                                    \"couldn't\": \"could not\", \"didn't\": \"did not\", \"doesn't\": \"does not\",\n",
    "                                    \"don't\": \"do not\", \"hadn't\": \"had not\",\n",
    "                                    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\", \"he'll\": \"he will\",\n",
    "                                    \"he's\": \"he is\",\n",
    "                                    \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\",\n",
    "                                    \"how's\": \"how is\", \"I'd\": \"I would\",\n",
    "                                    \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\", \"I'm\": \"I am\",\n",
    "                                    \"I've\": \"I have\",\n",
    "                                    \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",\n",
    "                                    \"i'll've\": \"i will have\", \"i'm\": \"i am\",\n",
    "                                    \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\",\n",
    "                                    \"it'll\": \"it will\",\n",
    "                                    \"it'll've\": \"it will have\", \"it's\": \"it is\", \"it’s\": \"it is\", \"let's\": \"let us\",\n",
    "                                    \"ma'am\": \"madam\", \"mayn't\": \"may not\",\n",
    "                                    \"might've\": \"might have\", \"mightn't\": \"might not\", \"mightn't've\": \"might not have\",\n",
    "                                    \"must've\": \"must have\",\n",
    "                                    \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n",
    "                                    \"needn't've\": \"need not have\",\n",
    "                                    \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\",\n",
    "                                    \"shan't\": \"shall not\",\n",
    "                                    \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\",\n",
    "                                    \"she'd've\": \"she would have\",\n",
    "                                    \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                                    \"should've\": \"should have\",\n",
    "                                    \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
    "                                    \"so's\": \"so as\",\n",
    "                                    \"this's\": \"this is\", \"that'd\": \"that would\", \"that'd've\": \"that would have\",\n",
    "                                    \"that's\": \"that is\",\n",
    "                                    \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\",\n",
    "                                    \"here's\": \"here is\",\n",
    "                                    \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\",\n",
    "                                    \"they'll've\": \"they will have\",\n",
    "                                    \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                                    \"wasn't\": \"was not\", \"we'd\": \"we would\",\n",
    "                                    \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\",\n",
    "                                    \"we're\": \"we are\", \"we've\": \"we have\",\n",
    "                                    \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\",\n",
    "                                    \"what're\": \"what are\",\n",
    "                                    \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\",\n",
    "                                    \"when've\": \"when have\", \"where'd\": \"where did\",\n",
    "                                    \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\",\n",
    "                                    \"who'll've\": \"who will have\",\n",
    "                                    \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\",\n",
    "                                    \"will've\": \"will have\",\n",
    "                                    \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\",\n",
    "                                    \"wouldn't\": \"would not\",\n",
    "                                    \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "                                    \"y'all'd've\": \"you all would have\",\n",
    "                                    \"y'all're\": \"you all are\", \"y'all've\": \"you all have\", \"you'd\": \"you would\",\n",
    "                                    \"you'd've\": \"you would have\",\n",
    "                                    \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\",\n",
    "                                    \"you've\": \"you have\", \"It's\": \"It is\", \"You'd\": \"You would\",\n",
    "                                    ' u ': \" you \", 'yrs': 'years', 'FYI': 'For your information', ' im ': ' I am ',\n",
    "                                    'lol': 'LOL', 'You\\'re': 'You are'\n",
    "            , 'can’t': 'can not', '…': '. ', '...': '. ', '\\'\\'': '\\'', '≠': '', 'ain’t': 'am not', 'I’m': 'I am',\n",
    "                                    'RT\\'s': ''}\n",
    "        self.emoticons = {\n",
    "            ':*': '<kiss>',\n",
    "            ':-*': '<kiss>',\n",
    "            ':x': '<kiss>',\n",
    "            ':-)': '<happy>',\n",
    "            ':-))': '<happy>',\n",
    "            ':-)))': '<happy>',\n",
    "            ':-))))': '<happy>',\n",
    "            ':-)))))': '<happy>',\n",
    "            ':-))))))': '<happy>',\n",
    "            ':)': '<happy>',\n",
    "            ':))': '<happy>',\n",
    "            ':)))': '<happy>',\n",
    "            ':))))': '<happy>',\n",
    "            ':)))))': '<happy>',\n",
    "            ':))))))': '<happy>',\n",
    "            ':)))))))': '<happy>',\n",
    "            ':o)': '<happy>',\n",
    "            ':]': '<happy>',\n",
    "            ':3': '<happy>',\n",
    "            ':c)': '<happy>',\n",
    "            ':>': '<happy>',\n",
    "            '=]': '<happy>',\n",
    "            '8)': '<happy>',\n",
    "            '=)': '<happy>',\n",
    "            ':}': '<happy>',\n",
    "            ':^)': '<happy>',\n",
    "            '|;-)': '<happy>',\n",
    "            \":'-)\": '<happy>',\n",
    "            \":')\": '<happy>',\n",
    "            '\\o/': '<happy>',\n",
    "            '*\\\\0/*': '<happy>',\n",
    "            ':-D': '<laugh>',\n",
    "            ':D': '<laugh>',\n",
    "            '8-D': '<laugh>',\n",
    "            '8D': '<laugh>',\n",
    "            'x-D': '<laugh>',\n",
    "            'xD': '<laugh>',\n",
    "            'X-D': '<laugh>',\n",
    "            'XD': '<laugh>',\n",
    "            '=-D': '<laugh>',\n",
    "            '=D': '<laugh>',\n",
    "            '=-3': '<laugh>',\n",
    "            '=3': '<laugh>',\n",
    "            'B^D': '<laugh>',\n",
    "            '>:[': '<sad>',\n",
    "            ':-(': '<sad>',\n",
    "            ':-((': '<sad>',\n",
    "            ':-(((': '<sad>',\n",
    "            ':-((((': '<sad>',\n",
    "            ':-(((((': '<sad>',\n",
    "            ':-((((((': '<sad>',\n",
    "            ':-(((((((': '<sad>',\n",
    "            ':(': '<sad>',\n",
    "            ':((': '<sad>',\n",
    "            ':(((': '<sad>',\n",
    "            ':((((': '<sad>',\n",
    "            ':(((((': '<sad>',\n",
    "            ':((((((': '<sad>',\n",
    "            ':(((((((': '<sad>',\n",
    "            ':((((((((': '<sad>',\n",
    "            ':-c': '<sad>',\n",
    "            ':c': '<sad>',\n",
    "            ':-<': '<sad>',\n",
    "            ':<': '<sad>',\n",
    "            ':-[': '<sad>',\n",
    "            ':[': '<sad>',\n",
    "            ':{': '<sad>',\n",
    "            ':-||': '<sad>',\n",
    "            ':@': '<sad>',\n",
    "            \":'-(\": '<sad>',\n",
    "            \":'(\": '<sad>',\n",
    "            'D:<': '<sad>',\n",
    "            'D:': '<sad>',\n",
    "            'D8': '<sad>',\n",
    "            'D;': '<sad>',\n",
    "            'D=': '<sad>',\n",
    "            'DX': '<sad>',\n",
    "            'v.v': '<sad>',\n",
    "            \"D-':\": '<sad>',\n",
    "            '(>_<)': '<sad>',\n",
    "            ':|': '<sad>',\n",
    "            '>:O': '<surprise>',\n",
    "            ':-O': '<surprise>',\n",
    "            ':-o': '<surprise>',\n",
    "            ':O': '<surprise>',\n",
    "            '°o°': '<surprise>',\n",
    "            'o_O': '<surprise>',\n",
    "            'o_0': '<surprise>',\n",
    "            'o.O': '<surprise>',\n",
    "            'o-o': '<surprise>',\n",
    "            '8-0': '<surprise>',\n",
    "            '|-O': '<surprise>',\n",
    "            ';-)': '<wink>',\n",
    "            ';)': '<wink>',\n",
    "            '*-)': '<wink>',\n",
    "            '*)': '<wink>',\n",
    "            ';-]': '<wink>',\n",
    "            ';]': '<wink>',\n",
    "            ';D': '<wink>',\n",
    "            ';^)': '<wink>',\n",
    "            ':-,': '<wink>',\n",
    "            '>:P': '<tong>',\n",
    "            ':-P': '<tong>',\n",
    "            ':P': '<tong>',\n",
    "            'X-P': '<tong>',\n",
    "            'x-p': '<tong>',\n",
    "            ':-p': '<tong>',\n",
    "            ':p': '<tong>',\n",
    "            '=p': '<tong>',\n",
    "            ':-Þ': '<tong>',\n",
    "            ':Þ': '<tong>',\n",
    "            ':-b': '<tong>',\n",
    "            ':b': '<tong>',\n",
    "            ':-&': '<tong>',\n",
    "            '>:\\\\': '<annoyed>',\n",
    "            '>:/': '<annoyed>',\n",
    "            ':-/': '<annoyed>',\n",
    "            ':-.': '<annoyed>',\n",
    "            ':/': '<annoyed>',\n",
    "            ':\\\\': '<annoyed>',\n",
    "            '=/': '<annoyed>',\n",
    "            '=\\\\': '<annoyed>',\n",
    "            ':L': '<annoyed>',\n",
    "            '=L': '<annoyed>',\n",
    "            ':S': '<annoyed>',\n",
    "            '>.<': '<annoyed>',\n",
    "            ':-|': '<annoyed>',\n",
    "            '<:-|': '<annoyed>',\n",
    "            ':-X': '<seallips>',\n",
    "            ':X': '<seallips>',\n",
    "            ':-#': '<seallips>',\n",
    "            ':#': '<seallips>',\n",
    "            'O:-)': '<angel>',\n",
    "            '0:-3': '<angel>',\n",
    "            '0:3': '<angel>',\n",
    "            '0:-)': '<angel>',\n",
    "            '0:)': '<angel>',\n",
    "            '0;^)': '<angel>',\n",
    "            '>:)': '<devil>',\n",
    "            '>:D': '<devil>',\n",
    "            '>:-D': '<devil>',\n",
    "            '>;)': '<devil>',\n",
    "            '>:-)': '<devil>',\n",
    "            '}:-)': '<devil>',\n",
    "            '}:)': '<devil>',\n",
    "            '3:-)': '<devil>',\n",
    "            '3:)': '<devil>',\n",
    "            'o/\\o': '<highfive>',\n",
    "            '^5': '<highfive>',\n",
    "            '>_>^': '<highfive>',\n",
    "            '^<_<': '<highfive>',\n",
    "            '<3': '<heart>',\n",
    "            '^3^': '<smile>',\n",
    "            \"(':\": '<smile>',\n",
    "            \" > < \": '<smile>',\n",
    "            \"UvU\": '<smile>',\n",
    "            \"uwu\": '<smile>',\n",
    "            'UwU': '<smile>'\n",
    "        }\n",
    "        \n",
    "        if emoticons_ignore:\n",
    "            for emotion_key in self.emoticons:\n",
    "                self.emoticons[emotion_key] = '<emoticon>'\n",
    "        \n",
    "        self.regex = self.get_compiled()\n",
    "\n",
    "    def get_compiled(self):\n",
    "        regexes = {k: re.compile(self.regex_dict[k]) for k, v in\n",
    "                   self.regex_dict.items()}\n",
    "        return regexes\n",
    "\n",
    "    def fit(self, Example):\n",
    "        for key, reg in self.regex.items():\n",
    "            Example = self.regex[key].sub(lambda m: \" <\" + key + \"> \",\n",
    "                                     Example)\n",
    "        for word in self.emoticons.keys():\n",
    "            Example = Example.replace(word, self.emoticons[word])\n",
    "        Example = Example.lower()\n",
    "        for word in self.contraction_mapping.keys():\n",
    "            Example = Example.replace(word, self.contraction_mapping[word])\n",
    "        Example = re.sub(r\"[\\-\\\"`@#$%^&*(|)/~\\[\\]{\\}:;+,._='!?]+\", \" \", Example)\n",
    "        Example = unicodedata.normalize('NFKD', Example).encode('ascii', errors='ignore').decode('utf8',\n",
    "                                                                                                 errors='ignore')\n",
    "        Example = re.sub(r'\\b([b-hB-Hj-zJ-Z] )', ' ', Example)\n",
    "        Example = re.sub(r'( [b-hB-Hj-zJ-Z])\\b', ' ', Example)\n",
    "        Example = ' '.join(Example.split())\n",
    "\n",
    "        return Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer = text.TfidfVectorizer(\n",
    "          analyzer='word', ngram_range=(1, 3),\n",
    "          min_df=2, use_idf=True, sublinear_tf=True)\n",
    "char_vectorizer = text.TfidfVectorizer(\n",
    "          analyzer='char', ngram_range=(3, 5),\n",
    "          min_df=2, use_idf=True, sublinear_tf=True)\n",
    "ngrams_vectorizer = Pipeline([('feats', FeatureUnion([('word_ngram', word_vectorizer),\n",
    "                ('char_ngram', char_vectorizer),\n",
    "                ])),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "free_text_converter = FreeTextConverter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/Train.csv\")\n",
    "test_df = pd.read_csv(\"data/Test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>TweetDate</th>\n",
       "      <th>TweetText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apple</td>\n",
       "      <td>positive</td>\n",
       "      <td>Tue Oct 18 21:53:25 +0000 2011</td>\n",
       "      <td>Now all @Apple has to do is get swype on the i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apple</td>\n",
       "      <td>positive</td>\n",
       "      <td>Tue Oct 18 21:09:33 +0000 2011</td>\n",
       "      <td>@Apple will be adding more carrier support to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>apple</td>\n",
       "      <td>positive</td>\n",
       "      <td>Tue Oct 18 21:02:20 +0000 2011</td>\n",
       "      <td>Hilarious @youtube video - guy does a duet wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>apple</td>\n",
       "      <td>positive</td>\n",
       "      <td>Tue Oct 18 20:40:10 +0000 2011</td>\n",
       "      <td>@RIM you made it too easy for me to switch to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>apple</td>\n",
       "      <td>positive</td>\n",
       "      <td>Tue Oct 18 20:34:00 +0000 2011</td>\n",
       "      <td>I just realized that the reason I got into twi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic Sentiment                       TweetDate  \\\n",
       "0  apple  positive  Tue Oct 18 21:53:25 +0000 2011   \n",
       "1  apple  positive  Tue Oct 18 21:09:33 +0000 2011   \n",
       "2  apple  positive  Tue Oct 18 21:02:20 +0000 2011   \n",
       "3  apple  positive  Tue Oct 18 20:40:10 +0000 2011   \n",
       "4  apple  positive  Tue Oct 18 20:34:00 +0000 2011   \n",
       "\n",
       "                                           TweetText  \n",
       "0  Now all @Apple has to do is get swype on the i...  \n",
       "1  @Apple will be adding more carrier support to ...  \n",
       "2  Hilarious @youtube video - guy does a duet wit...  \n",
       "3  @RIM you made it too easy for me to switch to ...  \n",
       "4  I just realized that the reason I got into twi...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сlassify it into 4 classes (+ irrelevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetClassifier:\n",
    "    DETECT_LANGUAGES = {\n",
    "        'en': 'english',\n",
    "        'ru': 'russian',\n",
    "        'az': 'azerbaijani',\n",
    "        'da': 'danish',\n",
    "        'de': 'german',\n",
    "        'el': 'greek',\n",
    "        'fr': 'french',\n",
    "        'fi': 'finnish',\n",
    "        'it': 'italian',\n",
    "        'ro': 'romanian',\n",
    "        'sl': 'slovene',\n",
    "        'es': 'spanish',\n",
    "        'sv': 'swedish',\n",
    "        'tr': 'turkish',\n",
    "        'hu': 'hungarian'\n",
    "    }\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.clf = LinearSVC()\n",
    "        self.free_text_converter = FreeTextConverter()\n",
    "        self.tweet_vectorizer = TweetTokenizer()\n",
    "        self.lexicon_dict = self._fill_lexicon()\n",
    "        \n",
    "    def _fill_lexicon(self):\n",
    "        lexicon_dict = {}\n",
    "        with open(\"assets/lexicon.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "            for line in file:\n",
    "                key, value = line.split(\"\\t\")\n",
    "                lexicon_dict[key] = value\n",
    "\n",
    "        return lexicon_dict\n",
    "    \n",
    "    def _preprocess_tweet(self, tweet, ignore_short_tweets=False):\n",
    "        converted_tweet = self.free_text_converter.fit(tweet)\n",
    "        tweet_tokens = self.tweet_vectorizer.tokenize(converted_tweet)\n",
    "        \n",
    "#         try:\n",
    "#             lang = detect(tweet)\n",
    "#             lang = self.DETECT_LANGUAGES.get(lang, 'english')\n",
    "#         except:\n",
    "#             lang = 'english'\n",
    "        \n",
    "#         stop_words = set(stopwords.words(lang))\n",
    "#         tweet_tokens = [word for word in tweet_tokens if word not in stop_words]\n",
    "\n",
    "        tweet_tokens = [self.lexicon_dict[word] if word in self.lexicon_dict else word for word in tweet_tokens]\n",
    "\n",
    "        if ignore_short_tweets and len(tweet_tokens) < 3:\n",
    "            return 'None'\n",
    "\n",
    "        return \" \".join(tweet_tokens)\n",
    "    \n",
    "    def _preprocess_train_tweet(self, tweet):\n",
    "        return self._preprocess_tweet(tweet, ignore_short_tweets=True)\n",
    "    \n",
    "    def _preprocess_test_tweet(self, tweet):\n",
    "        return self._preprocess_tweet(tweet)\n",
    "        \n",
    "    def fit(self, train_dataset_path, test_dataset_path=None):\n",
    "        # read datasets\n",
    "        train_df = pd.read_csv(train_dataset_path)\n",
    "        \n",
    "        # filter irrelevant tweets\n",
    "        train_df = train_df[train_df['Sentiment'] != 'irrelevant']\n",
    "        \n",
    "        # convert tweets\n",
    "        train_df['ConvertedTweet'] = train_df['TweetText'].apply(self._preprocess_train_tweet)\n",
    "        train_df['ConvertedTweet'] = train_df[train_df['ConvertedTweet'] != None]\n",
    "        \n",
    "        self.encoder=LabelEncoder()\n",
    "        self.encoder.fit(train_df['Sentiment'])\n",
    "        y_train = self.encoder.transform(train_df['Sentiment'])\n",
    "        \n",
    "        # vectorize\n",
    "        if test_dataset_path:\n",
    "            test_df = pd.read_csv(test_dataset_path)\n",
    "            test_df = test_df[test_df['Sentiment'] != 'irrelevant']\n",
    "            test_df['ConvertedTweet'] = test_df['TweetText'].apply(self.free_text_converter.fit)\n",
    "            self.vectorizer = ngrams_vectorizer.fit(pd.concat([train_df['ConvertedTweet'], test_df['ConvertedTweet']]))\n",
    "        else:\n",
    "            self.vectorizer = ngrams_vectorizer.fit(train_df['ConvertedTweet'])\n",
    "        x_train = self.vectorizer.transform(train_df['ConvertedTweet'])\n",
    "        \n",
    "        self.clf.fit(x_train, y_train)\n",
    "        \n",
    "    def predict(self, test_dataset_path, is_print_results=False):\n",
    "        test_df = pd.read_csv(test_dataset_path)\n",
    "        test_df = test_df[test_df['Sentiment'] != 'irrelevant']\n",
    "        test_df['ConvertedTweet'] = test_df['TweetText'].apply(self.free_text_converter.fit)\n",
    "        x_test = self.vectorizer.transform(test_df['ConvertedTweet'])\n",
    "        \n",
    "        y_pred = clf.predict(x_test)\n",
    "        \n",
    "        if is_print_results:\n",
    "            y_test = self.encoder.transform(test_df['Sentiment'])\n",
    "            print(classification_report(y_test, y_pred))\n",
    "        \n",
    "        return self.encoder.inverse_transform(y_pred)  \n",
    "    \n",
    "    def predict_tweets(self, tweets):\n",
    "        test_df = pd.DataFrame(tweets, columns=['TweetText'])\n",
    "        test_df['ConvertedTweet'] = test_df['TweetText'].apply(self.free_text_converter.fit)\n",
    "        x_test = self.vectorizer.transform(test_df['ConvertedTweet'])\n",
    "        y_pred = clf.predict(x_test)\n",
    "        return self.encoder.inverse_transform(y_pred)\n",
    "    \n",
    "    def _run(self, train_df, test_df, is_print_results=False):\n",
    "        train_df['ConvertedTweet'] = train_df['TweetText'].apply(self._preprocess_train_tweet)\n",
    "        train_df = train_df[train_df['ConvertedTweet'] != 'None']\n",
    "        test_df['ConvertedTweet'] = test_df['TweetText'].apply(self._preprocess_test_tweet)\n",
    "        \n",
    "        self.encoder=LabelEncoder()\n",
    "        self.encoder.fit(train_df['Sentiment'])\n",
    "        y_train = self.encoder.transform(train_df['Sentiment'])\n",
    "\n",
    "        self.vectorizer = ngrams_vectorizer.fit(train_df['ConvertedTweet'].values)\n",
    "        x_train = self.vectorizer.transform(train_df['ConvertedTweet'].values)\n",
    "        x_test = self.vectorizer.transform(test_df['ConvertedTweet'].values)\n",
    "        \n",
    "        self.clf.fit(x_train, y_train)\n",
    "        y_pred = self.clf.predict(x_test)\n",
    "        \n",
    "        if is_print_results:\n",
    "            y_test = self.encoder.transform(test_df['Sentiment'])\n",
    "            print(classification_report(y_test, y_pred))\n",
    "        \n",
    "        return self.encoder.inverse_transform(y_pred)\n",
    "    \n",
    "    def run(self, train_path=\"data/Train.csv\", test_path=\"data/Test.csv\", is_print_results=False):\n",
    "        train_df = pd.read_csv(train_path)\n",
    "        test_df = pd.read_csv(test_path)\n",
    "        train_df = train_df[train_df['Sentiment'] != 'irrelevant']\n",
    "        test_df = test_df[test_df['Sentiment'] != 'irrelevant']\n",
    "\n",
    "        return self._run(train_df, test_df, is_print_results)\n",
    "    \n",
    "    def run_with_tweets(self, tweets=[]):\n",
    "        train_df = pd.read_csv(\"data/Train.csv\")\n",
    "        train_df = train_df[train_df['Sentiment'] != 'irrelevant']\n",
    "\n",
    "        test_df = pd.DataFrame(tweets, columns=['TweetText'])\n",
    "        return self._run(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.51      0.60        49\n",
      "           1       0.81      0.91      0.86       156\n",
      "           2       0.67      0.56      0.61        32\n",
      "\n",
      "    accuracy                           0.78       237\n",
      "   macro avg       0.73      0.66      0.69       237\n",
      "weighted avg       0.77      0.78      0.77       237\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweet_clf = TweetClassifier()\n",
    "y_pred = tweet_clf.run(train_path=\"data/Train.csv\", test_path=\"data/Test.csv\", is_print_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify your own tweets\n",
    "##### Use pandas dataframe or tweet list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.51      0.60        49\n",
      "           1       0.81      0.91      0.86       156\n",
      "           2       0.67      0.56      0.61        32\n",
      "\n",
      "    accuracy                           0.78       237\n",
      "   macro avg       0.73      0.66      0.69       237\n",
      "weighted avg       0.77      0.78      0.77       237\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_path = \"data/Test.csv\"\n",
    "y_pred = tweet_clf.run(test_path=test_path, is_print_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neutral' 'positive' 'neutral' 'negative']\n"
     ]
    }
   ],
   "source": [
    "tweets = [\n",
    "    'Hey! Today I\\'m gonna sign a contract with Apple!',\n",
    "    'My mom made me Breakfast. I\\'m so happy ^^',\n",
    "    'SALE. -20% https://sales.com',\n",
    "    '@pepe I hate you!!!!'\n",
    "]\n",
    "\n",
    "y_pred = tweet_clf.run_with_tweets(tweets)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's predict organization by a Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all subsets given set\n",
    "from itertools import combinations\n",
    "def sub_lists(my_list):\n",
    "    subs = []\n",
    "    for i in range(0, len(my_list) + 1):\n",
    "        temp = [list(x) for x in combinations(my_list, i)]\n",
    "        if len(temp) > 0:\n",
    "            subs.extend(temp)\n",
    "    return subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_org(regex_ignore, print_result=False):\n",
    "    # read datasets\n",
    "    train_df = pd.read_csv(\"data/Train.csv\")\n",
    "    test_df = pd.read_csv(\"data/Test.csv\")\n",
    "\n",
    "    free_text_converter = FreeTextConverter(regex_ignore=regex_ignore)\n",
    "\n",
    "    train_df['ConvertedTweet']=train_df['TweetText'].apply(free_text_converter.fit)\n",
    "    test_df['ConvertedTweet']=test_df['TweetText'].apply(free_text_converter.fit)\n",
    "\n",
    "    vectorizer = ngrams_vectorizer.fit(train_df['ConvertedTweet'].values)\n",
    "    x_train = vectorizer.transform(train_df['ConvertedTweet'].values)\n",
    "    x_test = vectorizer.transform(test_df['ConvertedTweet'].values)\n",
    "\n",
    "    encoder=LabelEncoder()\n",
    "    y_train = encoder.fit_transform(train_df['Topic'])\n",
    "    y_test = encoder.fit_transform(test_df['Topic'])\n",
    "\n",
    "    clf = LinearSVC()\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "\n",
    "    if print_result:\n",
    "        print(classification_report(y_test, y_pred))\n",
    "    return y_test, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis: No need to convert hashtags and mentions, when we want to predict organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_list = list(FreeTextConverter().regex_dict.keys())\n",
    "scores = {}\n",
    "for regex_ignore in tqdm(sub_lists(pattern_list)):\n",
    "    y_test, y_pred = predict_org(regex_ignore=regex_ignore)\n",
    "    score = accuracy_score(y_test, y_pred)\n",
    "    scores[','.join(regex_ignore)] = score\n",
    "\n",
    "best_patterns = ''\n",
    "best_score = -1\n",
    "for key, score in scores.items():\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_patterns = key\n",
    "print('Best pattern subset and best accurancy score:')\n",
    "print(best_patterns, best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.96      0.94        98\n",
      "           1       0.85      0.77      0.81        79\n",
      "           2       0.85      0.74      0.79        78\n",
      "           3       0.74      0.86      0.80        87\n",
      "\n",
      "    accuracy                           0.84       342\n",
      "   macro avg       0.84      0.83      0.84       342\n",
      "weighted avg       0.85      0.84      0.84       342\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test, y_pred = predict_org(regex_ignore=['HASHTAG','MENTION'], print_result=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
